These parameters are used to enable the decoder to synchronise with the
incoming bytestream, and to indicate the parameters of the decoded
signal.

\textbf{Access Unit}

The Access Unit Header marks a point in the Bytestream from which
decoding may commence.

Sometimes sequences are simply played from start to finish. Often,
however, it is necessary to start playing a sequence part way through a
stream (for example if a viewer has just connected to a
broadcast/multicast transmission or following transmission errors). To
achieve this the player must be able to start decoding at some point in
the middle of a stream without requiring prior information.

Access Unit Header are points in a Dirac bit stream at which a player
can start decoding. An Access Unit Header provides the sequence and
decoding parameters with which to configure the decoder. The Access Unit
Header should be interpreted as giving an access point to the data in
display order.

An Access Unit Header does NOT imply that all subsequent frames in coded
order can be decoded. An Access Unit Header is followed immediately by
an Intra frame. However, typically, some Inter frames that are earlier
in display order, will be transmitted following an Intra frame. Such
Inter frames might be predicted from frames prior to the Access Unit
Header. Clearly such Inter frames cannot be decoded from only the
information following the Access Unit Header. However subsequent frames
can be decoded. Given a limited reordering depth in a prediction
structure, eventually all frames after a Access Unit Header will be
decodeable.

The parameters specified in the Access Unit Header remain the same
throughout a Video Sequence. That is, the parameters in later Access
Unit Headers simply repeat those in earlier headers. The repetitions are
included to provide entry points to start decoding the Bytestream.

See also Video Sequence

\textbf{Access Unit Parse Parameters}

The Access Unit Parse Parameters indicate the Picture Number which can
be decoded using the available information, and the Version Number,
Profile and Level of Dirac coding that has been used.

\textbf{Access Unit Picture Number}

The Access Unit Picture Number indicates the picture number of the first
picture that may be decoded following this Access Unit header.

\textbf{Aspect Ratios}

Aspect Ratios refers to the Pixel Aspect Ratios, not the image aspect
ratios. The Pixel Aspect Ratio value of an image is the ratio of the
intended spacing of horizontal samples (pixels) to the spacing of
vertical samples (picture lines) on the display device. Historically,
the aspect ratio was not square, and differed between 525-line and
625-line systems.

The shape of the elementary pixels is signalled as the quotient of two
variables, the numerator or dividend and denominator or divisor. The
numerator refers to the horizontal dimension and the denominator to the
vertical dimension.

Pixel Aspect Ratios are fundamental properties of sampled images because
they determine the displayed shape of objects in the image. Failure to
use the right parameters will result in distorted images, for example
circles will be displayed as ellipses etc. In spite of their fundamental
nature, pixel Aspect Ratios are not rigidly defined in many video
standards.

Some HDTV standards and computer image formats are defined to have Pixel
Aspect Ratios that are exactly 1:1.

Some video processing tools require an Image Aspect Ratio. This may be
derived from the pixel Aspect Ratio. The image aspect ratio is the ratio
of horizontal to vertical pixels multiplied by the pixel aspect ratio.
So, for example, for a 525 line 704 x 480 line picture the image aspect
ratio is $\frac{704 * 10}{480 * 11}$ which is exactly 4:3.

You are strongly advised to use one of the default Aspect Ratios.
However, if you know what you are doing and don't like the default
values you can, in principle, define your own. But our advice is don't
do it, just say no.

\textbf{Bytestream}

The Bytestream is the complete flow of transmitted Dirac data, after
coding.

\textbf{Chroma Excursion}

The dynamic range of the chrominance components.

\textbf{Chroma Format}

In sampled television signals, the signal is usually split into three
component: luminance and two colour components. The eye has around 20
times more receptors sensitive to the luminance of a scene than to the
chrominance. As a consequence, it is possible to provide less colour
detail than luminance detail and still satisfy the eye of the beholder.

The format for this subsampling is usually specified in the style 4:2:2
or 4:2:0 for example. This nomenclature may be written

luminance samples: chroma 1 samples : chroma 2 samples,

where chroma 1 and chroma 2 are the two colour components, usually $U$
and $V$ or $C_b$ and $C_r$. The figures refer to the number of samples
of each component along a small segment of the video line,
conventionally taking the number of luminance samples as 4. In the case
of 4:2:2, the signal would be $Y U Y V Y U Y V ...$., whilst in the case
of 4:2:0, it does not mean that there are no samples of $V$ at all; it
is just that on one line it is $YUYYUY ...$, and on the next it is
$YVYYVY ...$.

\textbf{Chroma Offset}

The zero level of the chrominance components.

\textbf{Clean Area}

Clean area is pure metadata, not used in any way to influence or control
the decoding process.

The Clean Area defines an area within which picture information is
subjectively uncontaminated by all edge effects. These may be transient
(and other) distortions. These may be coding artefacts, deliberate
introduction of elements such as time codes, etc.

Clean area is useful metadata for D-Cinema in which the 2K or 4K image
size is used as a container for a variety of picture formats with
different aspect ratios.  It is appropriate to display the Clean Area
rather than the whole picture.  The clean area is defined by the
position of the top left corner, and its horizontal and vertical
dimensions.

Clean area is desirable for ordinary SD video with a 720 pixel width
because only about 702 of the pixels represent active video, the
remainder allowing for overscan and filters falling off the edge of the
picture. The analogue PAL specification could be interpreted to say that
the clean width is 704 pixels. If you then use a pixel aspect ratio of
12:11 you end up with a picture aspect ratio of exactly 4:3. If you
could not specify the clean area then you would have big problems
specifying either the picture aspect ratio or the pixel aspect ratio. It
is simply wrong to say that the whole of the 720 pixels represent the
4:3 picture.

See also Clean Width, Clean Height, Left Offset, Top Offset.

\textbf{Clean Height}

The height of the Clean Area in pixels.

\textbf{Clean Width}

The width of the Clean Area in pixels.

\textbf{Colour Matrix}

We use the conventional relationships between the different
representations of the colour matrixes linking the different portrayals
of the colour channels. The $E_{Y}$, $E_{Cb}$, $E_{Cr}$ values are
derived from the $E_R$, $E_G$, $E_B$ values by the following equations.


The inverse transform, which would be used by a player, is the following.

Conversion between $E_R$, $E_G$, $E_B$ and $E_Y$, $E_{Cb}$, $E_{Cr}$ are
matrix operations. They can be specified by two of $K_R$, $K_G$, $K_B$.
The remaining $K$, say $K_Y$ can be calculated as $K_Y=1- K_R - K_B$.

\textbf{Colour Primaries}

The Colour Primaries define the assumed colours of the phosphors. The
choice offered is between three sets - the SMPTE C set, the EBU Tech
3213 set and the ITU Rec 709 set.

\textbf{Colour Space}

Dirac defines the subsets of colour primaries for the phosphors, the
colour matrixes and the transfer function used for the relationship
between signal level and light output.

We have deliberately eschewed provisions for a custom colour space.

Knowing the Source Parameters for the colour space, if the available
display has different parameters, it is possible to convert the received
signal to optimise the colour in the display.

All current video system use the following model for YUV coding of the
RGB values (computer systems often omit coding to and from YUV).

The R, G \& B are tristimulus values $(e.g. candelas/meter^{2})$. Their
relationship to CIE XYZ tristimulus values can be derived from the set
of primaries and white point defined in the colour primaries part of the
colour specification using the method described in SMPTE RP 177-1993. In
this document the RGB values are normalised to the range [0,1], so that
RGB=1,1,1 represents the peak white of the display device and RGB=0,0,0
represents black.

The $E_R$, $E_G$, $E_B$ values, also in the range [0,1], are related to
the RGB values by non-linear transfer functions $f()$ and $g()$. The
transfer function $f()$ is typically performed in the camera and is
specified in the Transfer Characteristic part of the Colour
Specification. For aesthetic and psychovisual reasons the transfer
function $g()$ is not quite the inverse of $f()$. In fact the combined
effect of $f()$ and $g()$ is such that

Where $r$ is the rendering intent or end to end gamma of the system,
which may vary between about 1.1 and 1.6 depending on viewing
conditions. The non-linear $E_R$, $E_G$, $E_B$ values are subject to a
matrix operation (known as non-constant luminance coding), which
transforms them into the $E_Y$, $E_{Cb}$, $E_{Cr}$ values. $E_Y$ is in
the range [0,1] and the $E_{Cb}$ and $E_{Cr}$ values are in the range
[-0.5, 0.5]. This is YUV coding and sometimes the U and V components are
subsampled, either horizontally or both horizontally and vertically. UV
sampling is specified by Colour Format.

Sometimes the matrix operation is omitted (or, equivalently, a unit
matrix is used). In this case $E_Y$, $E_{Cb}$, $E_{Cr}$ values are
simply the $E_Y$, $E_{Cb}$, $E_{Cr}$ values. This is signified by Colour
Format = RGB.

The $E_Y$, $E_{Cb}$, $E_{Cr}$ values are mapped to a range of integers
$Y$, $C_{b}$, $C_{r}$. Typically they are mapped to an 8 bit range [0,
255]. The way in which $E_Y$, $E_{Cb}$, $E_{Cr}$ values are mapped to
the integer values that are actually compressed is specified by Signal
Range.

\textbf{End of Sequence}

The End of Sequence code marks the end of a Video Sequence. It is not
necessarily the end of a complete work. The editing process may require
concatenation of Video Sequences. The decoder should therefore be
prepared to respond to data which follows the End of Sequence code.

\textbf{Field Dominance }

This is a flag which indicates that, for interlaced scanning, the top
field is first or second.

See also Interlace

\textbf{Frame Rate}

The Frame Rate is the rate at which frames of video are displayed.
Historically these have been close to the frequencies of the main
electricity supply: hence the global variations.

In Dirac, the frame rate is signalled as the quotient of two variables,
the numerator or dividend and denominator or divisor.

\textbf{Image Aspect Ratio}

The ratio of the horizontal to vertical dimensions of the Clean Area of
the image.

\textbf{Image Size}

The Image Size is a measure of the segmentation of the image. It is the
size of the image, measured in pixels of luminance. It is signalled as
the number of pixels along a line (the Luma Width) and the number of
lines in a frame (the Luma Height).

\textbf{Interlace}

If we have an interlaced source, each frame of the Video Sequence is
built up from two fields. These fields are spatially offset, with each
field delivering alternate lines of the final frame. For transmission,
the two fields can be combined, with field lines interleaved line by
line (pseudo-progressive format, the default for Dirac). Alteratively
they can be transmitted sequentially - interleaved field by field as in
conventional analogue broadcasting (giving low delay and low resource
coding).

The field interleaving flag indicates non-default field interleaving,
and the Sequential Fields (Boolean) parameter indicates whether the
fields are interleaved as pseudo-progressive or sequential fields.

With field sequential coding the picture sequence is a sequence of
fields rather than frames. So, for example, for interlaced 625 line
video we would have picture size 720 pixels by 288 lines, frame rate
25~Hz and Sequential Fields True.

Since Interlace is pure metadata there is no change to the coding
algorithm. It may seem unusual that the picture size refers to a field
rather than the whole frame.

However this preserves the separation of the Source Parameters, which
are pure metadata, from point of view of the decoding process.

When two fields are interlaced to make a frame, the top field (the odd
lines in the image) is usually transmitted first and the bottom field
(the even lines) comes second for images sourced in 625-line PAL. The
order is reversed for signals sourced from 525-line NTSC sources. The
difference arises because the image information starts on an odd line
number in PAL systems and an even line number in NTSC.

As an example, the default settings for the standard definition of SD
576 (the parameters which would be used as a basis for conventional PAL
in Europe) assume a pseudo-progressive or film mode, based on 25~Hz
progressive scan. If we wish to signal an interlaced signal, then it is
only necessary to modify the interlace flag. There needs to be a
sympathetic handling of the decoded signal when it is displayed. Whether
this is done by signalling or re-formatting is not a matter of
specification.

See also Scan Formats

\textbf{Left Offset}

The offset of the top left hand corner of the Clean Area from the left
hand side of the full image.

See also Clean Area

\textbf{Level}

The Level is an indication that the transmission is intended for a
decoder which may not necessarily decode the complete range of Video
Formats enabled by Dirac. Together with the Profile, it describes the
subset. A level is a set of decoder resource requirements that must be
satisfied in order to decode a bitstream, together with a set of
constraints on the bitstream that ensures that these requirements are
not exceeded.

This version of the specification does not define distinct levels and
profiles.  In future, we expect it to define the processing power of the
decoder - and hence the likely range of Video Formats which can be
handled by the device. One particular element we expect to be included
is the number of reference frames which can be stored.

Level is also used as a label for a parameter in the wavelet transform.
Hopefully there is no ambiguity caused by the use of the same name for
two different functions. The context should be clear.

See also Profile and Level in Section \ref{waveletparameters}

\textbf{Luma Excursion}

The dynamic range between black and white levels.

\textbf{Luma Offset}

The signal level corresponding to black level.

\textbf{Next Parse Offset }

Next Parse Offset is added to the Bytestream to simplify parsing. It
represents the offset in bytes from the start of the current Parse Info
to the start of the next Parse Info. So counting forward Next Parse
Offset bytes from the first byte (0x42 equivalent to B) of the current
Parse Info should yield a byte of value 0x42 or B corresponding to the
start of the next Parse Info. The Previous Parse Offset of the current
Parse Info equals the Next Parse Offset of the previous Parse Info.

\textbf{Parse Info}

Information which identifies the structure of the bytestream.

\textbf{Parse Info Prefix}

The Parse Info Prefix is the sequence of bytes 0x42  0x42  0x43  0x44,
which are the ASCII codes for BBCD.  This identifies the stream as a
Video Sequence coded using Dirac compression.

The Parse Info Prefix is present to allow an application to find a point
from which to start decoding. That is, the function of Parse Prefix
Header is to synchronise the decoder with the Bytestream. Decoding can
start from any Access Unit Header. The decoder first needs to find a
Parse Info structure. It should then check the Parse Code in the Parse
Info. If the following Parse Unit is an Access Unit Header then the
decoder can start decoding. If the Parse Unit is a Picture then the
decoder should skip forward by Next Parse Unit bytes (from the start of
the Parse Info Prefix) to the next Parse Info. The decoder would
continue skipping forward unit it locates an Access Unit Header. Note
that the decoder does not need to parse any Parse Units in order to
navigate through the stream to find an Access Unit Header. The Previous
Parse Offset is provided to allow searching backwards through the
Bytestream.

Any particular instance of the Parse Info Prefix in the Bytestream may
not, necessarily, indicate the start of a Parse Info structure. This is
because other parts of the Bytestream may, by chance, introduce these
bytes into the Bytestream. The use of arithmetic coding in Dirac means
that it is impossible to directly avoid accidentally introducing the
Parse Info Prefix.

When encoding a bytestream it is not necessary to avoid accidentally
introducing Parse Info Prefix sequences. They are present to allow
synchronisation of the bytes stream with the decoder and this can be
ensured, even in the presence of spurious Parse Info Prefixes, as
follows. When the decoder finds a Parse Info Prefix it should skip
forward by Next Parse Offset (or back by Previous Parse Offset) and
check whether the next three bytes are a Parse Info Prefix. If so the
decoder can be reasonably certain that it has found a genuine Parse Info
Prefix. If it does not find another Parse Info Prefix it was probably
unlucky enough to have found a spurious Parse Info Prefix. In this case
it should search for the next Prefix and repeat the test.

The probability of a spurious Parse Info Prefix is low; 1 in $2^{32}$
since the prefix is 4 bytes long. This is the probability of finding two
Parse Info Prefix sequences separated by Next Parse Offset. The test
outlined in the previous paragraph is, therefore, adequate in practice.
For the paranoid the test may be extended to find three Parse Info
Prefixes separated by the indicated Next Parse Offsets. This extended
test probably reduces the chance of failure to less than once in the
lifetime of the universe and should be sufficient for all but the
extremely cautious.

The test for two appropriately separated Parse Info Prefixes is, anyway,
prudent in any channel subject to bit errors even in the absence of
spurious Prefixes.

\textbf{Parse Unit}

The fundamental aggregation of data within the bytestream.

This definition of Dirac only includes two sorts of Parse Unit, Access
Unit Headers and Pictures. It is envisaged that other types of Parse
Unit may be introduced in future to carry data such as user data or
extension data.

\textbf{Picture Number}

Each picture has a unique Picture Number.

The Picture Number is a unique label (within the stream) indicating the
presentation/display ordering of the pictures. In a valid sequence the
Picture Numbers increment by one between consecutive frames (modulo
$2^{32}$, so 0x000 follows 0xFFFF).

If the Picture Number is even, and the picture is a field, then that
field has even parity. It is the first field of a pair of fields in a
frame. An interleaved Video Sequence coded sequentially (as opposed to
pseudo-progressively) starts with an even Picture Number. It does not
have to start with the Picture Number 0x000.

\textbf{Pixel Aspect Ratio}

See Aspect Ratio

\textbf{Previous Parse Offset}

The Previous Parse Offset is added to the Bytestream to simplify
parsing. It is the number of bytes backwards to the start of the
previous parse unit. The Previous Parse Offset of the current Parse Info
equals the Next Parse Offset of the previous

\textbf{Parse Info.}

See Next Parse Offset

\textbf{Profile}

The Profile is an indication that the transmission is intended for a
decoder which may not necessarily decode the complete range of Video
Formats enabled by Dirac. A profile is a set of decoding tools necessary
to decode a bit stream. A level is a set of decoder resource
requirements that must be satisfied in order to decode a bitstream,
together with a set of constraints on the bitstream that ensures that
these requirements are not exceeded.

This version of the specification does not define levels and profiles.

In future, we expect it to define the subset of tools that will be
accessible to the relevant decoder.

See also Level

\textbf{Scan Format}

In television, the picture is usually created by scanning the image as a
series of horizontal lines (some early formats used vertical lines, and
old electron beam devices had lines which were nearly, but not quite,
horizontal. We will ignore these in the Dirac specification).

A format in which all the lines in the frame are scanned in sequence is
called progressive format.

A format in which every other line in the frame is scanned, and then the
other half is called interleaved format. Each group of half lines is
called a field.  In old analogue broadcasting the two fields would be
broadcast one after the other: this is sequential field transmission.

It is also possible to combine the two fields to make a frame. This is
called pseudo-progressive format.

See also Interlace, Frame Rate

\textbf{Sequence Parameters}

A description of the parameters of the picture which are necessary to
decode the image. These include the Luma Width, Luma Height, Chroma
Format and Video Depth.  These parameters are essential to decoding and
displaying the bitstream.  The Sequence Parameters are intended to
change rarely if ever. If a change is necessary, it is recommended that
the bitstream is terminated by a Stop Sequence Parse Code and a new
bitstream is initiated.

See also Source Parameters

\textbf{Signal Range}

The Signal Range defines how the signal is scaled and clipped prior to
matrixing and display within the bits available, and is merely metadata
describing the source. It provides information to allow a bi-polar
signal such as $U$ and $V$ to be restored for display. Signal Range
embraces the set of parameters Luma Offset, Luma Excursion, Chroma
Offset and Chroma Excursion.

The offset and excursion values should be used to convert the
integer-valued decoded luma and chroma data $Y$, $C_{b}$, $C_{r}$ to
intermediate values $E_Y$, $E_{Cr}$, and $E_{Cb}$ by the recipe

$E_Y$, is normally clipped to the range [0,1], and $E_{Cr}$, and
$E_{Cb}$ to the range [-0.5,0.5].

This effectively clips

$Y$ to [LUMA\_OFFSET, LUMA\_OFFSET+LUMA\_EXCURSION]

and

$C_{b}$, $C_{r}$ to [CHROMA\_OFFSET-LUMA\_EXCURSION/2,
LUMA\_OFFSET+LUMA\_EXCURSION/2]

However, maintaining an extended RGB gamut may mean that either such
clipping is not done, or non-standard offset and excursion values are
used to extract the extended gamut from the non-negative decoded $Y$,
$C_{r}$, and $C_{b}$ values.

See also Luma Offset, Luma Excursion, Chroma Offset and Chroma Excursion.

\textbf{Source Parameters}

Source Parameters are a description of the parameters of the picture
which are not necessary to decode the image, but which may be desirable
to ensure accurate display of the decoded images. These include elements
such as Frame Rate, interlace information, Pixel Aspect Ratios,
information about the clean area, luma and chroma parameters and the
colour system being used.

The interpretation of Source Parameters by a display mechanism
interfacing with a compliant decoder is not specified. However, it would
make jolly good sense to follow the recommendations and interpretations
signalled if at all possible.

Likewise, encoders should ensure that accurate Source Parameter
information is encoded to maximise the potential quality of displayed
video.

See also Sequence Parameters

\textbf{Stream}

A Stream is a concatenation of Video Sequences.

See also Video Sequence

\textbf{Top Offset}

The Top Offset is the offset of the top left hand corner of the Clean
Area from the top of the full image.

See also Clean Area

\textbf{Transfer Function}

The Transfer Function defines the non-linear processing used in the
camera when converting the received light flux into an electrical
signal.

\textbf{Version Number}

The Version Number is coded as an unsigned integer with the first minor
version starting at zero.

Dirac is expected to be released in different versions at different
times. Version numbering will have two elements: the major version
number and the minor version number. Later versions will have the higher
numbers. Small changes in specification will be indicated by increases
in the minor version number.

The major version number defines the version of the syntax with which
the bit steam complies. Decoders that comply with a version of the spec
must be able to parse all previous versions too. Decoders that comply
with a version of the spec may not be able to parse the bit stream
corresponding to a later spec.

The first major version starts at one. Major version zero is a draft.
All minor versions of a spec should be functionally compatible with
earlier minor versions with the same major version number. Later minor
versions may contain corrections, clarifications, disambiguations etc;
they must not contain new features.

The first minor version starts at zero.

\textbf{Video Depth}

Video Depth is the number of bits used to represent the video data, i.e.
the video word width (typically 8  or 10 bits).

Video Depth is different from the Signal Range. The former defines how
many bits are used to contain the signal, and is used in the decoding
process. The input data, be it $Y$, $U$, $V$, or $R$, $G$ and $B$ are
all coded as if they were unsigned integers.

Note, this is separate from what the bits represent. It would be
possible, for example, to have an 8 bit signal represented in a 10 bit
word (in which case either the upper two, or lower two, bits of the word
would always be zero). The meaning of the bits is defined in the Signal
Range (the Luma and Chroma Offsets and Excursions) part of the Source
Parameters. Video Depth relates to how the video is coded. The Signal
Range relates to what the numbers mean and how the video should be
displayed.

\textbf{Video Format}

The Video Format indicates whether the signal conforms to something
close to one of the conventional video formats (such as High Definition,
PAL, NTSC, QCIF etc) or whether it is a custom format, with all
parameters available for setting independently. The video format
embraces a range of Source Parameters and Sequence

\textbf{Parameters.}

See also Source Parameters, Sequence Parameters and Appendix XXX [the
default format settings]

\textbf{Video Sequence}

A Video Sequence is a collection of images which can be of any length,
which have constant Source Parameters (e.g. picture size, aspect ratio
etc.) If the parameters need to change the only way to do it is to
signal the end of a Video Sequence and start a new Video Sequence.

The process of editing two coded sequences together might introduce
presentation order picture numbers which are not contiguous. This is
accommodated by introducing an End of Sequence Parse Code before a cut
so that the decoder would restart after a cut.
